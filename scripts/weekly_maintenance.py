"""
ì£¼ê°„ ìœ ì§€ë³´ìˆ˜ ìŠ¤í¬ë¦½íŠ¸
ë§¤ì£¼ ì¼ìš”ì¼ ìì • ì‹¤í–‰ (í¬ë¡ ì¡)

ê¸°ëŠ¥:
1. í”„ë¡œí† ì½œ ì •ë¦¬ (ì €ì‚¬ìš© ì•½ì–´ ì‚­ì œ, ìœ ì‚¬ ì•½ì–´ í†µí•©)
2. ë¡œê·¸ íŒŒì¼ ì •ë¦¬
3. í†µê³„ ì§‘ê³„
4. ë°±ì—… ê´€ë¦¬
"""
import sys
import os
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from datetime import datetime, timedelta
from ai.protocols.protocol_pruning import protocol_pruner
from utils.logger import info, warning


def run_weekly_maintenance():
    """ì£¼ê°„ ìœ ì§€ë³´ìˆ˜ ë©”ì¸ í•¨ìˆ˜"""
    info("\n" + "=" * 60)
    info("ğŸ”§ ì£¼ê°„ ìœ ì§€ë³´ìˆ˜ ì‹œì‘")
    info(f"   ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    info("=" * 60 + "\n")

    try:
        # 1. í”„ë¡œí† ì½œ ì •ë¦¬
        info("ğŸ“‹ ì‘ì—… 1/4: í”„ë¡œí† ì½œ ì •ë¦¬")
        protocol_pruner.prune_protocol()

        # 2. ì˜¤ë˜ëœ ë¡œê·¸ ì •ë¦¬
        info("\nğŸ“‹ ì‘ì—… 2/4: ë¡œê·¸ íŒŒì¼ ì •ë¦¬")
        cleanup_old_logs(days=30)

        # 3. ì˜¤ë˜ëœ ë°±ì—… ì •ë¦¬
        info("\nğŸ“‹ ì‘ì—… 3/4: ë°±ì—… íŒŒì¼ ì •ë¦¬")
        cleanup_old_backups(keep_count=50)

        # 4. ì£¼ê°„ í†µê³„ ìƒì„±
        info("\nğŸ“‹ ì‘ì—… 4/4: ì£¼ê°„ í†µê³„ ìƒì„±")
        generate_weekly_stats()

        info("\n" + "=" * 60)
        info("âœ… ì£¼ê°„ ìœ ì§€ë³´ìˆ˜ ì™„ë£Œ")
        info("=" * 60 + "\n")

        return True

    except Exception as e:
        warning(f"\nâŒ ìœ ì§€ë³´ìˆ˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False


def cleanup_old_logs(days=30):
    """ì˜¤ë˜ëœ ë¡œê·¸ íŒŒì¼ ì •ë¦¬"""
    log_dir = Path("data/logs")

    if not log_dir.exists():
        info("   ë¡œê·¸ í´ë” ì—†ìŒ - ìŠ¤í‚µ")
        return

    cutoff_date = datetime.now() - timedelta(days=days)
    deleted_count = 0

    for log_file in log_dir.glob("*.log"):
        # íŒŒì¼ ìˆ˜ì • ì‹œê°„ ì²´í¬
        mtime = datetime.fromtimestamp(log_file.stat().st_mtime)

        if mtime < cutoff_date:
            log_file.unlink()
            deleted_count += 1
            info(f"   ğŸ—‘ï¸ ì‚­ì œ: {log_file.name}")

    if deleted_count == 0:
        info(f"   âœ… ì •ë¦¬ ë¶ˆí•„ìš” (ëª¨ë“  ë¡œê·¸ê°€ {days}ì¼ ì´ë‚´)")
    else:
        info(f"   âœ… {deleted_count}ê°œ ë¡œê·¸ íŒŒì¼ ì‚­ì œ")


def cleanup_old_backups(keep_count=50):
    """ì˜¤ë˜ëœ ë°±ì—… íŒŒì¼ ì •ë¦¬ (ìµœê·¼ Nê°œë§Œ ìœ ì§€)"""
    backup_dir = Path("data/protocol_backups")

    if not backup_dir.exists():
        info("   ë°±ì—… í´ë” ì—†ìŒ - ìŠ¤í‚µ")
        return

    # ë°±ì—… íŒŒì¼ ëª©ë¡ (ì‹œê°„ìˆœ ì •ë ¬)
    backups = sorted(
        backup_dir.glob("*.json"),
        key=lambda x: x.stat().st_mtime,
        reverse=True  # ìµœì‹ ìˆœ
    )

    # ì˜¤ë˜ëœ ê²ƒ ì‚­ì œ
    deleted_count = 0
    for backup in backups[keep_count:]:
        backup.unlink()
        deleted_count += 1
        info(f"   ğŸ—‘ï¸ ì‚­ì œ: {backup.name}")

    if deleted_count == 0:
        info(f"   âœ… ì •ë¦¬ ë¶ˆí•„ìš” (ë°±ì—… {len(backups)}ê°œ â‰¤ {keep_count}ê°œ)")
    else:
        info(f"   âœ… {deleted_count}ê°œ ë°±ì—… íŒŒì¼ ì‚­ì œ (ìµœê·¼ {keep_count}ê°œ ìœ ì§€)")


def generate_weekly_stats():
    """ì£¼ê°„ í†µê³„ ìƒì„±"""
    try:
        import json
        from pathlib import Path

        protocol_file = Path("data/dynamic_protocol.json")

        if not protocol_file.exists():
            info("   í”„ë¡œí† ì½œ íŒŒì¼ ì—†ìŒ - ìŠ¤í‚µ")
            return

        with open(protocol_file, 'r', encoding='utf-8') as f:
            protocol = json.load(f)

        # í†µê³„
        total_abbrs = len(protocol.get('dynamic_abbreviations', {}))
        total_evolutions = len(protocol.get('evolution_history', []))
        version = protocol.get('version', 'N/A')

        # ê°€ì¥ ë§ì´ ì“°ì¸ ì•½ì–´
        abbrs = protocol.get('dynamic_abbreviations', {})
        top_5 = sorted(
            abbrs.items(),
            key=lambda x: x[1].get('usage_count', 0),
            reverse=True
        )[:5]

        # ì¶œë ¥
        info(f"\n   ğŸ“Š í”„ë¡œí† ì½œ í†µê³„:")
        info(f"      ë²„ì „: v{version}")
        info(f"      ì•½ì–´ ìˆ˜: {total_abbrs}ê°œ")
        info(f"      ì§„í™” íšŸìˆ˜: {total_evolutions}íšŒ")

        if top_5:
            info(f"\n   ğŸ† TOP 5 ì•½ì–´:")
            for i, (abbr, data) in enumerate(top_5, 1):
                usage = data.get('usage_count', 0)
                meaning = data.get('meaning', 'N/A')
                info(f"      {i}. {abbr} = {meaning} ({usage}íšŒ)")

        # í†µê³„ íŒŒì¼ ì €ì¥
        stats_dir = Path("data/weekly_stats")
        stats_dir.mkdir(exist_ok=True)

        stats_file = stats_dir / f"stats_{datetime.now().strftime('%Y%m%d')}.json"

        stats = {
            'date': datetime.now().isoformat(),
            'version': version,
            'total_abbreviations': total_abbrs,
            'total_evolutions': total_evolutions,
            'top_5_abbreviations': [
                {
                    'abbr': abbr,
                    'meaning': data.get('meaning'),
                    'usage_count': data.get('usage_count', 0)
                }
                for abbr, data in top_5
            ]
        }

        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)

        info(f"\n   ğŸ’¾ í†µê³„ ì €ì¥: {stats_file.name}")

    except Exception as e:
        warning(f"   âš ï¸ í†µê³„ ìƒì„± ì‹¤íŒ¨: {e}")


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    success = run_weekly_maintenance()

    if success:
        exit(0)  # ì„±ê³µ
    else:
        exit(1)  # ì‹¤íŒ¨


if __name__ == "__main__":
    main()